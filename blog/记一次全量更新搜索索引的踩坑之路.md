# 记一次全量更新搜索索引的踩坑之路


首先是拿到这个任务的光荣的任务的时候，我有点懵，之前不太了解这块东西。然后看了下脚本，只有短短的几行代码，看着还挺简单的。动手了，才知道，此事必有蹊跷。

### 采用多线程

这个方案在一开始就被我pass掉了，因为python的多线程其实挺吃屎的，他需要不断地切换上下文，访问同一对象的时候，还会锁住，造成堵塞。而且，在我们的hero服务器上，16核去跑多线程，怎么说也说不过去吧，你这性能没有用到位呀。

### 采用多进程的方式

首先，python的多进程在我理解是伪多进程，这个我也是踩完了坑才知道的。


1. 采用了多进程的队列

	这是我最开始采用的办法，大致流程如下:
		
		class Consumer(multiprocessing.Process):
		    def __init__(self, task_queue, result_queue):
		        multiprocessing.Process.__init__(self)
		        self.task_queue = task_queue
		        self.result_queue = result_queue
		
		    def run(self):
		        while True:
		            next_task = self.task_queue.get()
		            if next_task is None:
		                self.task_queue.task_done()
		                return
		            matches = next_task
		            self.task_queue.task_done()
		            self.result_queue.put(matches)
		            
		tasks = multiprocessing.JoinableQueue()
	    results = multiprocessing.Queue()
	
	    num_consumers = 4
	
	    consumers = [Consumer(tasks, results) for i in range(num_consumers)]
	    [consumer.start() for consumer in consumers]
	
	    [tasks.put(i) for i in range(100)]
	    [tasks.put(None) for i in range(num_consumers)]
	
	    tasks.join()
	
	    while not results.empty():
	        print results.get()
	    results.close()
	    
	实现的逻辑其实很简单，就是我开十个进程，然后各自的进程去从商品列表中取数据，执行完之后，将数据汇总，输出到csv文件中去。这样就能比之前的顺序执行大概快个六七倍的样子。
	
	但是呢， 理想很丰满，现实很骨感。在实际的操作过程中，我总是发现，卡主不动了，那时候我的第一想法是，完了，死锁了。但是呢，什么地方产生的死锁呢，我定位不出来，可能是我使用队列的姿势不对，所以我换了另一种更加简单的方式，那就是采用进程池子。
	

2. 采用进程池 multiprocess pool
	
		def task(data):
		    print 'run task: %s, %s' % (data, os.getpid())
		    start = time.time()
		    time.sleep(random.random() * 3)
		    end = time.time()
		    print 'Task %s runs %0.2f seconds.' % (data, (end - start))
		    
		p = multiprocessing.Pool()
	    [p.apply_async(task, (i,)) for i in range(10)]
	    p.close()
	    p.join()
	    print 'pool end...'
	    
	可以看出，使用进程池管理进程的方式，要不上面使用队列要方便很多，因此，相对来说，也更加方便我们去定位死锁的问题。然后，就是一步一步的debug，因为采用了进程池的方式，依然会产生死锁。即使只使用两个进程，然后我一步一步的跟进进程中需要做的一些步骤，即函数调用的时候，分别都做了哪些事情。
	
	其实，做得事情还蛮多的：
		
		* 获取商品的id列表
		* 根据商品的id列表去获取商品的实例（如果有缓存，就不走数据库流程）
		* 期间还会执行一遍添加到缓存的操作
		* 然后去获取商品对应的分类等，并刷新一遍缓存
		* 把这些商品的集合给整合在一起
		* 然后将生成的csv文件，上传到solr服务器
		* 最后更新solr

### 解决办法

如何定位到呢，首先，我做了去数据库的操作，即只采用缓存已有的数据，我发现，并不会产生死锁问题。然后呢，就单纯的使用数据库访问，发现会死锁，那么问题就找到了，是因为并发访问数据库的时候，会产生死锁的现象。google了一下后，发现pymysql的thread为1，即一次只支持一个进程访问，在访问的时候会生成一个描述符，当多个进程并发访问的时候，就会产生资源的竞争，因此产生了死锁。
	
问题找到了，那么怎么解决呢。
	
首先，效率最低的方案显然是去网上找教程，去解决，如何在多进程的状态下，pymysql不会死锁，因为，我没有深入研究过pymysql，对这些也是一知半解，而既然pymysql设计成这样子，就应该有它的道理。所以，我决定换一种思路。

为什么我们这么多的脚本，都用到了sqlstore里的store对象，却没有产生死锁呢，我觉得应该是因为实例的对象是不同的吧，因此，我只要new10个store对象，不就可以完美解决这个问题了么。然后，我把更新商品的所有索引的方法全部都拿出来，整合在了同一个地方，然后不同的对象，配合上不同的store对象，果然，死锁的问题就解决了。

随之而来的问题是什么呢，那就是重复代码的问题，本来我们可以采用已有的一些方法，快速的实现更新商品索引的需求，但是呢，现在这个方案其实相当于是copy&paste，当然中间经过了一定的加工，代码可读性也没问题，但总归感觉不是很理想。然后我左思右想，在叔叔的提点下，才有了现在这个最终的方案。

### 最终方案

其实更新商品索引的问题，我们可以简化成：

	1. 取商品
	2. 将商品记录到csv文件
	3. 上传到solr服务器，并更新

耗时的地方在这个取商品上，即我们现在的数据库有大概170万的商品，如果顺序去做的话，时间的耗费一定是非常巨大的，因此，我们为什么不把这些商品给拆分成十个商品区间，即0-10， 10-20这样，然后再crontab中启动写十个任务，后面指定区间的编号，这样既解决了死锁的问题，又能够重用已有的代码，避免产生一些未知的问题。

采用这个方法，搜索索引的更新速度提高了6.5倍，现在只需要一个小时，还是非常可观的。


###最后的最后

通过这个需求，我学到了挺多东西的，比如如何使用python的多进程，了解了pymysql的一些设计，使用不当的话，容易产生死锁。这些问题，在顺序执行的时候，我们是无法接触并学习到的。

最后就是，解题的思路很重要。很多看似很简单的解题思路，其实非常的有效。所谓大道至简，说的就是这个吧。
	


	
	 